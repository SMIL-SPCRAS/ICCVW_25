# ICCVW 2025

# ğŸ§  9th_ABAW Compound Expression (CE) Recognition Challenge

This repository contains a modular pipeline for multimodal emotion recognition using **audio**, **video**, **text**, and **scene** modalities. It is structured to support isolated training and evaluation within each modality, as well as ensemble and fusion-based methods.

## âš™ï¸ Modalities
- [ğŸ§  LLMs promting](./LLMs/)
- [ğŸµ Audio modality](./src/audio/)
- [ğŸ”€ Feature-level fusion](./src/fusion/)
- [ğŸ“„ Text modality](./src/text/)
- [ğŸ¤– Scene modality](./src/text_llm/)
- [ğŸ¥ Video modality and probability-level fusion](./src/video/)

## ğŸ“ Project Structure
```
LLMs # LLMs promting and LLMs results
src
â”œâ”€â”€ audio/ # Audio-based emotion recognition, data loading and preprocessing.
â”œâ”€â”€ common/ # Shared utilities and trainer logic across modalities.
â”œâ”€â”€ fusion/ # Feature-level fusion and data preparation.
â”œâ”€â”€ text/ # Text-based emotion recognition, data loading and preprocessing.
â”œâ”€â”€ text_llm/ # LLM-based emotion recognition or Scene modality, data loading and preprocessing.
â”œâ”€â”€ video/ # Video-based emotion recognition, data loading and preprocessing, as well as probability-level fusion.
```

## ğŸš€ Usage
Each module is (mostly) self-contained. To train or evaluate, navigate to the corresponding directory and run the appropriate script (e.g., `train.py`, `inference.py`, or `notebooks/` for LLM-based modelling).

Example (from `text/` modality):
```bash
cd text
python train.py
```