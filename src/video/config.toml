# ---------------------------
# Dataset settings
# --------------------------

[datasets.afew]
task = "emotions"
base_dir = "E:/Databases/9th_ABAW/AFEW/Chunk/"
csv_path = "{base_dir}/{split}_union.csv"
video_dir  = "{base_dir}/{split}/"

#[datasets.c_expr_db]
#task = "emotions"
#base_dir = "E:/Databases/9th_ABAW/C-EXPR-DB/Chunk/"
#csv_path = "{base_dir}/{split}_union.csv"
#video_dir  = "{base_dir}/{split}/"

[datasets.affwild2]
task = "emotions"
base_dir = "E:/Databases/9th_ABAW/AffWild2/Chunk/"
csv_path = "{base_dir}/{split}_union.csv"
video_dir  = "{base_dir}/{split}/"

# ---------------------------
# List of emotions
# ---------------------------
emotion_columns = ["Neutral","Anger","Disgust","Fear","Happiness","Sadness","Surprise","Other"]


# ---------------------------
# DataLoader parameters
# ---------------------------
[dataloader]
num_workers = 0
shuffle = true
prepare_only = false

# ---------------------------
# General training parameters
# ---------------------------
[train.general]
random_seed = 42        # fix random seed for reproducibility (0 = different each time)
subset_size = 0         # limit on number of examples (0 = use entire dataset)
merge_probability = 0   # percentage of short files to merge
batch_size = 64         # batch size
num_epochs = 100        # number of training epochs
max_patience = 25       # maximum number of epochs without improvements (for Early Stopping)
save_best_model = true
save_prepared_data = true         # save extracted features (embeddings)
save_feature_path = './features/' # path for saving embeddings
search_type = "greedy"            # search strategy: "greedy", "exhaustive" or "none"
path_to_df_ls = ''                # path to dataframe with softened labels - Qwen3-4B_emotions_union or Phi-4-mini-instruct_emotions_union
smoothing_probability = 0.0       # percentage of using softened labels
opt_set = 'test'                  # dataset for optimizing training parameters

# ---------------------------
# Model parameters
# ---------------------------
[train.model]
model_name = "VideoMamba"  # model name (VideoFormer, VideoMamba)
hidden_dim = 512           # hidden state size
num_transformer_heads = 4   # number of attention heads in transformer
tr_layer_number = 1         # number of transformer layers
mamba_d_state = 4          # state size in Mamba
mamba_ker_size = 6          # kernel size in Mamba
mamba_layer_number = 4      # number of Mamba layers
positional_encoding = false # whether to use positional encoding
dropout = 0.15              # dropout between layers
out_features = 128          # final feature size before classification
mode = 'mean'               # feature aggregation method (e.g. "mean", "max", etc.)

# ---------------------------
# Optimizer parameters
# ---------------------------
[train.optimizer]
optimizer = "adam"        # optimizer type: "adam", "adamw", "lion", "sgd", "rmsprop"
lr = 1e-4                 # initial learning rate
weight_decay = 0.0        # weight decay for regularization
momentum = 0.9            # momentum (used only in SGD)

# ---------------------------
# Scheduler parameters
# ---------------------------
[train.scheduler]
scheduler_type = "plateau" # scheduler type: "none", "plateau", "cosine", "onecycle" or HuggingFace-style ("huggingface_linear", "huggingface_cosine" "huggingface_cosine_with_restarts" etc.)
warmup_ratio = 0.1         # ratio of warmup iterations to total steps (0.1 = 10%)

[embeddings]
image_classifier_checkpoint = "torchscript_model_0_66_37_wo_gl.pth"
image_model_type = "clip"  # resnet18, emo, emoresnet50, clip
image_embedding_dim = 512   # 2048 (emoresnet50) and 512 (resnet18, emo, clip) video embedding dimension
cut_target_layer = 3 # 4 (emoresnet50) and 2 (resnet18) and 3 (emo) layer to cut the model and extract features
roi_video = "face" # region of interest or entire scene # face or scene
counter_need_frames = 20 # how many frames to select from all possible with uniform step
image_size = 224 # image width and height
emb_normalize = false  # whether to normalize vector with L2-norm
device = "cuda"          # "cuda" or "cpu", where to load model