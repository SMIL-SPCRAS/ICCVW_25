{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6111c0ac-74ce-4e54-9515-d61e4562d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def draw_box(image, box, color=(255, 0, 255)):\n",
    "    \"\"\"Draw a rectangle on the image.\"\"\"\n",
    "    line_width = 2\n",
    "    lw = line_width or max(round(sum(image.shape) / 2 * 0.003), 2)\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
    "\n",
    "def norm_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \"\"\"Convert normalized coordinates to pixel coordinates.\"\"\"\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    \n",
    "    return x_px, y_px\n",
    "\n",
    "def get_box(fl, w, h, off_y):\n",
    "    \"\"\"Get bounding box coordinates from lip landmarks.\"\"\"\n",
    "    lips = np.asarray([61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 409, 270, 269, 267, 0, 37, 39, 40, 185]).reshape(1,-1)\n",
    "    idx_to_coors = {\n",
    "        idx: norm_coordinates(landmark.x, landmark.y, w, h)\n",
    "        for idx, landmark in enumerate(fl.landmark)\n",
    "    }\n",
    "    \n",
    "    x_min = np.min(np.asarray(list(idx_to_coors.values()))[lips][:, :, 0])\n",
    "    y_min = np.min(np.asarray(list(idx_to_coors.values()))[lips][:, :, 1])\n",
    "    x_max = np.max(np.asarray(list(idx_to_coors.values()))[lips][:, :, 0])\n",
    "    y_max = np.max(np.asarray(list(idx_to_coors.values()))[lips][:, :, 1])\n",
    "\n",
    "    upper_lip = 13\n",
    "    lower_lip = 14\n",
    "\n",
    "    upper_lip_y = off_y + idx_to_coors[upper_lip][1]\n",
    "    lower_lip_y = off_y + idx_to_coors[lower_lip][1]\n",
    "    diff = lower_lip_y - upper_lip_y\n",
    "\n",
    "    return (max(0, x_min), max(0, y_min), min(w - 1, x_max), min(h - 1, y_max)), 0 if diff < 0 else diff\n",
    "\n",
    "def pth_processing(fp):\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def init(self):\n",
    "            super(PreprocessInput, self).init()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        \n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0).to('cuda')\n",
    "        return img\n",
    "    return get_img_torch(fp)\n",
    "\n",
    "def get_metadata(video_path, video_name):\n",
    "\n",
    "    model = YOLO('yolov11n-face.pt')\n",
    "  \n",
    "    DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    counter = 1\n",
    "    \n",
    "    embeds = []\n",
    "    fr_all = []\n",
    "    embeds_all = []\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(\n",
    "            static_image_mode=True,\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "        ) as face_mesh:\n",
    "    \n",
    "        while True:\n",
    "            ret, im0 = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            results = model.track(im0, persist=True, imgsz=640, conf=0.01, iou=0.5, augment=False, device='cuda', verbose=False)\n",
    "    \n",
    "            idx_face, idx_box, startX, startY, endX, endY, startX_mouth, startY_mouth, endX_mouth, endY_mouth, diff, label = None, None, None, None, None, None, None, None, None, None, None, None\n",
    "    \n",
    "            output = [None, None, None, None, None, None, None]\n",
    "    \n",
    "            for idx, i in enumerate(results[0].boxes):\n",
    "                box = i.xyxy.int().cpu().tolist()[0]\n",
    "                idx_box = i.id.int().cpu().tolist()[0] if i.id else -1\n",
    "                idx_face = idx\n",
    "                \n",
    "                startX = box[0]  \n",
    "                startY = box[1]\n",
    "                endX = box[2] \n",
    "                endY = box[3]\n",
    "\n",
    "                startX_new = max(0, startX-int(startX*0.1))\n",
    "                startY_new = max(0, startY-int(startY*0.1))\n",
    "                endX_new = min(w, endX+int(startX*0.1))\n",
    "                endY_new = min(h, endY+int(startY*0.1))\n",
    "                \n",
    "                cur_fr = im0[startY_new: endY_new, startX_new: endX_new]\n",
    "                if cur_fr.shape[0] > 0 and cur_fr.shape[1] > 0:\n",
    "                    cur_fr = cv2.cvtColor(cur_fr, cv2.COLOR_BGR2RGB)\n",
    "                    results = face_mesh.process(cur_fr)\n",
    "                    if results.multi_face_landmarks:\n",
    "                        face_landmarks = results.multi_face_landmarks[0]\n",
    "                        box_lips, diff = get_box(face_landmarks, cur_fr.shape[1], cur_fr.shape[0], off_y=startY-int(startY*0.1))\n",
    "                        startX_mouth = startX-int(startX*0.1)+box_lips[0]\n",
    "                        startY_mouth = startY-int(startY*0.1)+box_lips[1]\n",
    "                        endX_mouth = startX-int(startX*0.1)+box_lips[2]\n",
    "                        endY_mouth = startY-int(startY*0.1)+box_lips[3]\n",
    "        \n",
    "                        cur_fr = pth_processing(Image.fromarray(cur_fr))\n",
    "\n",
    "                        fr_all.append(cur_fr)\n",
    "\n",
    "                        embeds.append([video_name, counter, idx_box, idx_face, startX, startY, endX, endY, startX_mouth, startY_mouth, endX_mouth, endY_mouth, diff])\n",
    "            if diff is None:\n",
    "                embeds.append([video_name, counter, idx_box, idx_face, startX, startY, endX, endY, startX_mouth, startY_mouth, endX_mouth, endY_mouth, diff])\n",
    "                fr_all.append(torch.zeros((1, 3, 224, 224)).to('cuda'))\n",
    "            counter += 1\n",
    "            \n",
    "    for start in range(0, len(fr_all), 70):\n",
    "        end = start + 70\n",
    "        curr_fr_all = fr_all[start:end]\n",
    "        curr_fr_all = torch.cat(curr_fr_all, dim=0)\n",
    "        outputs = pth_model(curr_fr_all)\n",
    "        outputs = torch.nn.functional.softmax(outputs, dim=1).cpu().detach().numpy().tolist()\n",
    "        for idx_frame, idx_out in zip(range(end), range(len(outputs))):\n",
    "            cl = np.argmax(outputs[idx_out])\n",
    "            label = DICT_EMO[cl]\n",
    "            embeds_all.append([*embeds[idx_frame], *outputs[idx_out], label])\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    df_segments = pd.DataFrame(embeds_all, columns=[\"video_name\", \"frame\", \"face_id\", \"box_id\", \"startX_face\", \"startY_face\", \"endX_face\", \"endY_face\", \"startX_mouth\", \"startY_mouth\", \"endX_mouth\", \"endY_mouth\", \"diff_lips\", \"neutral_prob\", \"happiness_prob\", 'sadness_prob', 'surprise_prob', 'fear_prob', 'disgust_prob', 'anger_prob', 'pred_emotion'])\n",
    "\n",
    "    return df_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e8302-81da-4c7d-92af-d62b9446c635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                | 9/411 [00:18<11:58,  1.79s/it]"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "name = '0_66_37_wo_gl'\n",
    "pth_model = torch.jit.load('torchscript_model_{0}.pth'.format(name)).to('cuda')\n",
    "pth_model.eval()\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "corpus = \"AFEW\"\n",
    "path = f\"E:/Databases/9th_ABAW/{corpus}/Chunk\"\n",
    "subsets = [\"dev\", \"train\"]\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for subset in subsets:\n",
    "    video_names = os.listdir(f'{path}/{subset}/')\n",
    "    for video_name in tqdm(video_names):\n",
    "        curr_path = f\"{path}/{subset}/{video_name}\"\n",
    "        curr_df = get_metadata(curr_path, video_name)\n",
    "        combined_df = pd.concat([combined_df, curr_df], ignore_index=True)\n",
    "                   \n",
    "    combined_df.to_csv(os.path.join(path, f'{subset}_faces.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db534759-63ff-481c-9bd1-a36cc203d130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
